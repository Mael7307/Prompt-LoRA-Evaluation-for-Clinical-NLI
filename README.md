# ðŸ§  ClinicalReasonBench: Prompt & LoRA Evaluation for Clinical NLI

This repository implements the experimental framework from our AAAI paper:

> **Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies**  
> [[[paper link]](https://arxiv.org/abs/2507.04142)] | BibTeX: *(see below)*

We provide a unified pipeline for evaluating structured prompting strategies and parameter-efficient fine-tuning (LoRA) across multiple compact language models on clinical Natural Language Inference (NLI) tasks, including NLI4CT, MedNLI, and TREC.

---

## ðŸ“‹ Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
  - [Inference](#inference)
  - [Training (LoRA)](#training-lora)
  - [Evaluation](#evaluation)
- [Project Structure](#project-structure)
- [Reproducibility](#reproducibility)
- [Citation](#citation)
- [License](#license)

---

## âœ¨ Features

- âœ… Modular inference, training, and evaluation bash pipelines
- âœ… Support for 4 reasoning-focused prompting strategies: NLR (CoT), ISRR, TAR (ReAct), SSR (QuaSAR)
- âœ… LoRA-based fine-tuning for efficient adaptation of 1.5â€“3.8B models
- âœ… Support for MedNLI and TREC for generalization evaluation
- âœ… Fully automated Apptainer-based container setup

---

## ðŸ› ï¸ Installation

### 1. Clone this repo
```bash
git clone https://github.com/Mael7307/Prompt-LoRA-Evaluation-for-Clinical-NLI.git
cd Prompt-LoRA-Evaluation-for-Clinical-NLI
````

### 2. Build the Apptainer container

```bash
sbatch build_apptainer.sh
```

This will produce `SACNLI.sif`, used by all scripts.

---
ðŸ”‘ Dataset Access
ðŸ“Œ MedNLI (not redistributed)
The MedNLI dataset cannot be distributed with this repository.
To access it, please request it directly from the authors via PhysioNet:

â†’ https://physionet.org/content/mednli/1.0.0/

Once downloaded, place the MedNLI dataset in a local directory (e.g., ./MedNLI/) as expected by the scripts.

## ðŸš€ Usage

### ðŸ” Inference

Run predictions across:

* All models (`llama`, `qwen`, `phi`, `deepseek`)
* All prompt types (QS, CoT, REACT, scrit)
* All datasets (NLI4CT, MedNLI, TREC)
* With and without LoRA

```bash
bash inference.sh
```

---

### ðŸ‹ï¸ Training (LoRA)

Fine-tunes models on NLI4CT subsets using LoRA adapters.

```bash
sbatch train.sh
```

LoRA weights are saved in `lora_outputs/`.

---

### ðŸ“Š Evaluation

Evaluates predictions across all datasets and saves metrics.

```bash
bash evaluate.sh
```

Results go to `results/`.

---

## ðŸ“ Project Structure

```
.
â”œâ”€â”€ prompts/                 # Prompt templates
â”œâ”€â”€ src/                     # Core Python scripts
â”‚   â”œâ”€â”€ mednli_inference.py
â”‚   â”œâ”€â”€ nli4ct_inference.py
â”‚   â”œâ”€â”€ evaluate_mednli.py
â”‚   â”œâ”€â”€ evaluate_trec.py
â”‚   â””â”€â”€ train.py
â”œâ”€â”€ results/                 # Results
â”œâ”€â”€ lora_outputs/            # Trained LoRA adapters
â”œâ”€â”€ inference.sh             # Inference pipeline
â”œâ”€â”€ train.sh                 # Training pipeline (SLURM)
â”œâ”€â”€ evaluate.sh              # Evaluation script
â””â”€â”€ build_apptainer.sh       # Apptainer container build
```

---

## ðŸ” Reproducibility

To reproduce experiments:

```bash
sbatch build_apptainer.sh     # 1. Build container
sbatch train.sh               # 2. Train LoRA adapters
bash inference.sh             # 3. Run inference
bash evaluate.sh              # 4. Evaluate outputs
```

---

## ðŸ“„ Citation

If you use this code or dataset, please cite our AAAI paper:

> **Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies**

```bibtex
@article{jullien2025dissecting,
  title={Dissecting Clinical Reasoning in Language Models: A Comparative Study of Prompts and Model Adaptation Strategies},
  author={Jullien, Mael and Valentino, Marco and Ranaldi, Leonardo and Freitas, Andre},
  journal={arXiv preprint arXiv:2507.04142},
  year={2025}
}
```
